# 21-10-04 - Lecture 1

CI583 - Lecture run by James Burton <J.Burton@brighton.ac.uk>

## Description

This module is taught in 1 semester, and is divided into two parts; Data
Structures and Algorithms, then Operating Systems. These two are connected, as
operating systems rely on data structures and algorithms for their design.

![](img/2021-10-04-10-06-13.png)

Extension of CI401 - data structures lead on directly from programming. THey are
fundamentally problem solving tools. CI553 (not taking) extends this with OOP.

![](img/2021-10-04-10-07-38.png)

Choosing the right data structure and strategy for a problem is the bulk of the
work of a solution.

![](img/2021-10-04-10-08-34.png)

We will be writing our own data structures instead of using the standard
library, to learn the implementation of these. These things have already been
written, but they don't properly expose their inner workings.

![](img/2021-10-04-10-09-25.png)

"I don't think that you truly understand the concept without writing it from
scratch" - *Burton*. Deep understanding comes from implementation and
development, rather than utilisation.

This implementation issue also helps to teach

![](img/2021-10-04-10-11-35.png)

Learning outcomes:
- Assess how choices in data structures impacts performance.
- Choose appropriate data structures and algorithms when designing applications.
- Write programs using object-oriented design principles.
- Solve problems using data structures such as linked lists, stacks, queues,
  hash tables, binary trees, heaps, binary search trees.
- Solve problems using algorithm design methods such as the greedy method,
  divide and conquer, dynamic programming, backtracking.

![](img/2021-10-04-10-13-27.png)

Weekly lecture on Monday at 10am-12am.

Lab session on Thursday at 3pm-4pm. Also time for assignment and questions face
to face.

![](img/2021-10-04-10-14-27.png)

Independent study is important: at least 4 hours a week. Organisation is
self-directed, and without it you will likely fall behind.

![](img/2021-10-04-10-18-03.png)

Assessment is 100% based on coursework.

## Overview

![](img/2021-10-04-10-19-16.png)

Data Structure Types:
- Linked Lists
- Arrays
- Stacks
- Queues
- Trees
  - Binary Trees
  - Search Trees

![](img/2021-10-04-10-21-14.png)

Searches, sorting, and selecting are most important - Lots of complexity in
programs comes from these three things with various data structures.

i.e. Binary search is much less complex in Big-O notation than linear search.

Chance can be exponentially complex, but it can also be very useful for elegant algorithms.

![](img/2021-10-04-10-23-23.png)
![](img/2021-10-04-10-25-40.png)

Resources above

<https://coursera.org/course/algo>
<https://www-cs-faculty.stanford.edu/~uno/taocp.html>

For data structure visualisations:
<https://www.cs.usfca.edu/~galles/visualization/Algorithms.html>

## Implementation

![](img/2021-10-04-10-35-41.png)

`abstract` in Java means `static`; used for other classes to inherit/extend the
original class. Anything that extends `Shape` has a method `area()` by default,
requiring each inherited class to define it's own method.

![](img/2021-10-04-10-38-26.png)

Arrays have a fixed size, and, depending on the language, must be of the same
type. This differs from tuples and lists. Arrays allow access using an index,
starting at 0 (in most languages). Think of an array like a series of letter
boxes.

**snip** basic stuff... See [Further Reading](#further-reading)

## Calculating Complexity

![](img/2021-10-04-11-27-58.png)

You always take the worst-case complexity for determining Big-O notation: in
this example, it'd be O(n^2) complexity.

![](img/2021-10-04-11-29-32.png)

![](img/2021-10-04-11-30-00.png)

Not too sure where he's going with this...

![](img/2021-10-04-11-30-54.png)

Oh okay. Not sure why this needed a bigger explanation.

![](img/2021-10-04-11-31-22.png)

Standard practise is to strip the constants as they don't provide any
information.

![](img/2021-10-04-11-32-35.png)

This shows that functions `A` and `B` have identical complexities, even if they
have different constants; both algorithms are O(n^2).

![](img/2021-10-04-11-33-31.png)

Big-O notation (at most as fast as), Big-Omega notation (at least as fast as),
Big-Theta notation (at the same rate)

- Ω(f) - Best case
- O(f) - Worst case
- Θ(f) - Same case

![](img/2021-10-04-11-36-23.png)

`let n=2^k - 1` for some `k`

After each loop, the power of 2 decreases by 1.

Therefore: the complexity is O(lg(n)), which is equivalent to O(log(n)).

![](img/2021-10-04-11-39-56.png)

Exponential growth is (generally) **BAD** for performance and scalability.

- Fixed time is great
- Logarithmic time is good
- Linear time is ok!
- Log linear time O(n log n) is pretty alright
- Quadratic time O(n^2) is bad
- Exponential time O(2^n) is **BAD**
- Factorial time O(n!) is **terrible** and is not good for performance at all. Avoid.

## Further Reading

Read about:
- Data Structure Types
  - Array
    - Hash table
  - Stack
  - Queue
  - Linked List
  - Graph
  - Tree
- Big O Notation
  - Time complexity
  - Data complexity
  - Big Theta
  - Big Omega
  - Mathematical operations
    - Logarithms
      - Natural Logs
    - Factorials
    - Floor/Ceil operations
    - Summations
